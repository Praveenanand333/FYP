{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># mean_0_a</th>\n",
       "      <th>mean_1_a</th>\n",
       "      <th>mean_2_a</th>\n",
       "      <th>mean_3_a</th>\n",
       "      <th>mean_4_a</th>\n",
       "      <th>mean_d_0_a</th>\n",
       "      <th>mean_d_1_a</th>\n",
       "      <th>mean_d_2_a</th>\n",
       "      <th>mean_d_3_a</th>\n",
       "      <th>mean_d_4_a</th>\n",
       "      <th>...</th>\n",
       "      <th>fft_741_b</th>\n",
       "      <th>fft_742_b</th>\n",
       "      <th>fft_743_b</th>\n",
       "      <th>fft_744_b</th>\n",
       "      <th>fft_745_b</th>\n",
       "      <th>fft_746_b</th>\n",
       "      <th>fft_747_b</th>\n",
       "      <th>fft_748_b</th>\n",
       "      <th>fft_749_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.62</td>\n",
       "      <td>30.3</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>26.3</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-15.70</td>\n",
       "      <td>2.06</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>23.5</td>\n",
       "      <td>20.3</td>\n",
       "      <td>20.3</td>\n",
       "      <td>23.5</td>\n",
       "      <td>-215.0</td>\n",
       "      <td>280.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>280.00</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.80</td>\n",
       "      <td>33.1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>22.8</td>\n",
       "      <td>6.550</td>\n",
       "      <td>1.680</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.83</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>182.0</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>2.57</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.90</td>\n",
       "      <td>29.4</td>\n",
       "      <td>-416.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>23.7</td>\n",
       "      <td>79.900</td>\n",
       "      <td>3.360</td>\n",
       "      <td>90.20</td>\n",
       "      <td>89.90</td>\n",
       "      <td>2.03</td>\n",
       "      <td>...</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-267.0</td>\n",
       "      <td>281.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>281.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.90</td>\n",
       "      <td>31.6</td>\n",
       "      <td>-143.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>24.3</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>8.82</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>...</td>\n",
       "      <td>299.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>9.53</td>\n",
       "      <td>9.53</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.30</td>\n",
       "      <td>31.3</td>\n",
       "      <td>45.2</td>\n",
       "      <td>27.3</td>\n",
       "      <td>24.5</td>\n",
       "      <td>34.800</td>\n",
       "      <td>-5.790</td>\n",
       "      <td>3.06</td>\n",
       "      <td>41.40</td>\n",
       "      <td>5.52</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.1</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.90</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  mean_d_1_a  \\\n",
       "0        4.62      30.3    -356.0      15.6      26.3       1.070       0.411   \n",
       "1       28.80      33.1      32.0      25.8      22.8       6.550       1.680   \n",
       "2        8.90      29.4    -416.0      16.7      23.7      79.900       3.360   \n",
       "3       14.90      31.6    -143.0      19.8      24.3      -0.584      -0.284   \n",
       "4       28.30      31.3      45.2      27.3      24.5      34.800      -5.790   \n",
       "\n",
       "   mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  fft_742_b  fft_743_b  \\\n",
       "0      -15.70        2.06        3.15  ...       23.5       20.3       20.3   \n",
       "1        2.88        3.83       -4.82  ...      -23.3      -21.8      -21.8   \n",
       "2       90.20       89.90        2.03  ...      462.0     -233.0     -233.0   \n",
       "3        8.82        2.30       -1.97  ...      299.0     -243.0     -243.0   \n",
       "4        3.06       41.40        5.52  ...       12.0       38.1       38.1   \n",
       "\n",
       "   fft_744_b  fft_745_b  fft_746_b  fft_747_b  fft_748_b  fft_749_b     label  \n",
       "0       23.5     -215.0     280.00    -162.00    -162.00     280.00  NEGATIVE  \n",
       "1      -23.3      182.0       2.57     -31.60     -31.60       2.57   NEUTRAL  \n",
       "2      462.0     -267.0     281.00    -148.00    -148.00     281.00  POSITIVE  \n",
       "3      299.0      132.0     -12.40       9.53       9.53     -12.40  POSITIVE  \n",
       "4       12.0      119.0     -17.60      23.90      23.90     -17.60   NEUTRAL  \n",
       "\n",
       "[5 rows x 2549 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('preprocessed_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['# mean_0_a', 'mean_1_a', 'mean_2_a', 'mean_3_a', 'mean_4_a',\n",
       "       'mean_d_0_a', 'mean_d_1_a', 'mean_d_2_a', 'mean_d_3_a', 'mean_d_4_a',\n",
       "       ...\n",
       "       'fft_741_b', 'fft_742_b', 'fft_743_b', 'fft_744_b', 'fft_745_b',\n",
       "       'fft_746_b', 'fft_747_b', 'fft_748_b', 'fft_749_b', 'label'],\n",
       "      dtype='object', length=2549)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Praveen\\AppData\\Local\\Temp\\ipykernel_17004\\3254061002.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_encoded = df.replace(encode)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='label', data=df)\n",
    "\n",
    "# df.isnull().sum().sum()\n",
    "\n",
    "encode = ({'NEUTRAL': 0, 'POSITIVE': 1, 'NEGATIVE': 0} )\n",
    "df_encoded = df.replace(encode)\n",
    "\n",
    "X=df_encoded.drop([\"label\"]  ,axis=1)\n",
    "y = df_encoded.loc[:,'label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2132, 2548)\n",
      "(2132, 2)\n"
     ]
    }
   ],
   "source": [
    "X_orig=df_encoded.drop([\"label\"]  ,axis=1)\n",
    "y_orig = to_categorical(df_encoded.loc[:,'label'].values)\n",
    "print(X_orig.shape)\n",
    "print(y_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from keras.utils import to_categorical\n",
    "\n",
    "# # Assuming df_encoded is your dataframe with 'label' column and features\n",
    "# X_orig = df_encoded.drop([\"label\"], axis=1).values\n",
    "# y_orig = to_categorical(df_encoded['label'].values)\n",
    "\n",
    "# # Split the dataset into training and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_orig, y_orig, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Hopfield Network class\n",
    "# class HopfieldNetwork:\n",
    "#     def __init__(self, num_units):\n",
    "#         self.num_units = num_units\n",
    "#         self.weights = np.zeros((num_units, num_units))\n",
    "\n",
    "#     def train(self, patterns):\n",
    "#         for pattern in patterns:\n",
    "#             # Outer product to form the weight matrix\n",
    "#             self.weights += np.outer(pattern, pattern)\n",
    "        \n",
    "#         # Set the diagonal to 0 to avoid self-connections\n",
    "#         np.fill_diagonal(self.weights, 0)\n",
    "\n",
    "#     def recall(self, pattern, steps=5):\n",
    "#         # Update the pattern for a number of steps\n",
    "#         for _ in range(steps):\n",
    "#             pattern = self.activation_function(np.dot(pattern, self.weights))\n",
    "#         return pattern\n",
    "\n",
    "#     @staticmethod\n",
    "#     def activation_function(x):\n",
    "#         # Activation function (sign function)\n",
    "#         return np.where(x >= 0, 1, -1)\n",
    "\n",
    "# # Prepare binary training patterns from X_train (input features)\n",
    "# # Convert input features to bipolar format for Hopfield training\n",
    "# train_patterns = []\n",
    "# for i in range(X_train.shape[0]):\n",
    "#     pattern = (X_train[i] * 2) - 1  # Convert to bipolar (-1, 1)\n",
    "#     train_patterns.append(pattern)\n",
    "\n",
    "# train_patterns = np.array(train_patterns)\n",
    "\n",
    "# # Create and train the Hopfield Network with the input features\n",
    "# hopfield_net = HopfieldNetwork(num_units=X_train.shape[1])  # Number of input features\n",
    "# hopfield_net.train(train_patterns)\n",
    "\n",
    "# # Prepare test set for evaluation\n",
    "# y_pred = []\n",
    "# for test_instance in X_test:\n",
    "#     # Convert the test instance to bipolar representation\n",
    "#     bipolar_test_instance = (test_instance * 2) - 1  # Convert to bipolar (-1, 1)\n",
    "    \n",
    "#     # Recall the closest stored pattern\n",
    "#     recalled_pattern = hopfield_net.recall(bipolar_test_instance)\n",
    "#     y_pred.append(recalled_pattern)\n",
    "\n",
    "# # Convert predictions back to original format (non-bipolar)\n",
    "# y_pred = np.array(y_pred)\n",
    "\n",
    "# # For classification, you will need to map these recalled patterns back to the original label space.\n",
    "# # You could cluster the recalled patterns or use another method to convert them back to labels.\n",
    "# # In this simplified example, we'll compare recalled patterns directly to the training patterns.\n",
    "\n",
    "# # Calculate accuracy based on how close recalled patterns are to the original patterns\n",
    "# y_pred_classes = []\n",
    "# for recalled_pattern in y_pred:\n",
    "#     # Find the closest matching pattern in the training set\n",
    "#     distances = np.sum(np.abs(train_patterns - recalled_pattern), axis=1)\n",
    "#     closest_pattern_idx = np.argmin(distances)\n",
    "    \n",
    "#     # Use the label of the closest pattern\n",
    "#     predicted_class = np.argmax(y_train[closest_pattern_idx])  # Assuming labels are one-hot encoded\n",
    "#     y_pred_classes.append(predicted_class)\n",
    "\n",
    "# # Convert predictions and true labels for accuracy and confusion matrix\n",
    "# y_pred_classes = np.array(y_pred_classes)\n",
    "# expected_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# # Calculate accuracy\n",
    "# accuracy = accuracy_score(expected_classes, y_pred_classes)\n",
    "# print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# # Confusion matrix\n",
    "# conf_matrix = confusion_matrix(expected_classes, y_pred_classes)\n",
    "# print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch_geometric.nn import GCNConv\n",
    "# from torch_geometric.data import Data\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from torch_geometric.utils import dense_to_sparse\n",
    "\n",
    "\n",
    "\n",
    "# # Convert the data to PyTorch tensors\n",
    "# X_tensor = torch.tensor(X_orig.values, dtype=torch.float32)\n",
    "# y_tensor = torch.tensor(np.argmax(y_orig, axis=1), dtype=torch.long)  # Convert one-hot to class indices\n",
    "\n",
    "# # Split the data\n",
    "# X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor = train_test_split(\n",
    "#     X_tensor, y_tensor,\n",
    "#     test_size=0.2,\n",
    "#     stratify=y_tensor,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # Create edge_index for the training data\n",
    "# num_train_nodes = X_train_tensor.shape[0]\n",
    "# adjacency_matrix_train = torch.ones(num_train_nodes, num_train_nodes)  # Fully connected\n",
    "# edge_index_train, _ = dense_to_sparse(adjacency_matrix_train)\n",
    "\n",
    "# # Create edge_index for the test data\n",
    "# num_test_nodes = X_test_tensor.shape[0]\n",
    "# adjacency_matrix_test = torch.ones(num_test_nodes, num_test_nodes)  # Fully connected\n",
    "# edge_index_test, _ = dense_to_sparse(adjacency_matrix_test)\n",
    "\n",
    "# # Define the GCN Model\n",
    "# class ComplexGCN(torch.nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3, dropout_rate=0.5):\n",
    "#         super(ComplexGCN, self).__init__()\n",
    "#         self.num_layers = num_layers\n",
    "#         self.convs = torch.nn.ModuleList()\n",
    "#         self.bns = torch.nn.ModuleList()\n",
    "        \n",
    "#         # First layer\n",
    "#         self.convs.append(GCNConv(input_dim, hidden_dim))\n",
    "#         self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "        \n",
    "#         # Hidden layers\n",
    "#         for _ in range(num_layers - 2):\n",
    "#             self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "#             self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "        \n",
    "#         # Final layer\n",
    "#         self.convs.append(GCNConv(hidden_dim, output_dim))\n",
    "#         self.dropout_rate = dropout_rate\n",
    "\n",
    "#     def forward(self, data):\n",
    "#         x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "#         for i in range(self.num_layers - 1):\n",
    "#             x = self.convs[i](x, edge_index)\n",
    "#             x = self.bns[i](x)\n",
    "#             x = F.relu(x)\n",
    "#             x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        \n",
    "#         # Final layer (without dropout)\n",
    "#         x = self.convs[-1](x, edge_index)\n",
    "#         return F.log_softmax(x, dim=1)\n",
    "\n",
    "# # Create data objects for PyG\n",
    "# data_train = Data(x=X_train_tensor, edge_index=edge_index_train, y=y_train_tensor)\n",
    "# data_test = Data(x=X_test_tensor, edge_index=edge_index_test, y=y_test_tensor)\n",
    "\n",
    "# # Define model parameters\n",
    "# input_dim = X_train_tensor.shape[1]\n",
    "# hidden_dim = 64  # You can adjust this\n",
    "# output_dim = len(torch.unique(y_train_tensor))  # Number of classes\n",
    "\n",
    "# # Initialize the model, optimizer, and loss function\n",
    "# model = ComplexGCN(input_dim, hidden_dim, output_dim, num_layers=3, dropout_rate=0.5)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# # Training Loop\n",
    "# def train(data):\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     out = model(data)\n",
    "#     loss = loss_fn(out, data.y)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     return loss.item()\n",
    "\n",
    "# # Evaluate the model\n",
    "# def evaluate(data):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         out = model(data)\n",
    "#     return out\n",
    "\n",
    "# # Training\n",
    "# num_epochs = 20\n",
    "# for epoch in range(num_epochs):\n",
    "#     train_loss = train(data_train)\n",
    "#     print(f'Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}')\n",
    "\n",
    "# # Testing the model\n",
    "# output = evaluate(data_test)\n",
    "# pred_classes = output.argmax(dim=1)  # Get predicted classes\n",
    "\n",
    "# # Calculate accuracy\n",
    "# accuracy = (pred_classes == data_test.y).float().mean()\n",
    "# print(f'Accuracy: {accuracy.item():.4f}')\n",
    "\n",
    "# # Confusion Matrix\n",
    "# expected_classes = data_test.y\n",
    "# conf_matrix = confusion_matrix(expected_classes.cpu().numpy(), pred_classes.cpu().numpy())\n",
    "# print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential, Model\n",
    "# from tensorflow.keras.layers import Dense, Input\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_orig, y_orig, test_size=0.2, random_state=42)\n",
    "\n",
    "# def get_feature_extractor(input_shape):\n",
    "#     model = Sequential([\n",
    "#         Dense(1024, activation='relu', input_shape=input_shape),\n",
    "#         Dense(512, activation='relu'),\n",
    "#         Dense(256, activation='relu'),\n",
    "#         Dense(128, activation='relu')  \n",
    "#     ])\n",
    "#     return model\n",
    "\n",
    "# input_shape = (X_train.shape[1],)  \n",
    "# nn_model = get_feature_extractor(input_shape)\n",
    "\n",
    "# nn_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# X_train_features = nn_model.predict(X_train)\n",
    "# X_test_features = nn_model.predict(X_test)\n",
    "\n",
    "# print(\"Extracted features shape:\", X_train_features.shape)  \n",
    "\n",
    "# y_train_svm = np.argmax(y_train, axis=1)\n",
    "# y_test_svm = np.argmax(y_test, axis=1)\n",
    "# svm_model = SVC(kernel='linear', C=1.0)\n",
    "# svm_model.fit(X_train_features, y_train_svm)\n",
    "\n",
    "# y_pred = svm_model.predict(X_test_features)\n",
    "\n",
    "# accuracy = accuracy_score(y_test_svm, y_pred)\n",
    "# conf_matrix = confusion_matrix(y_test_svm, y_pred)\n",
    "\n",
    "# print(f\"SVM Accuracy: {accuracy * 100:.2f}%\")\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train  shape: (1705, 1, 2548)\n",
      "X_test  shape: (427, 1, 2548)\n",
      "(1705, 2)\n",
      "(427, 2)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "y = to_categorical(y)\n",
    "# print(y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "# print(X_train.shape)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0],1,X.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0],1,X.shape[1]))\n",
    "print(f\"X_train  shape: {X_train.shape}\")\n",
    "print(f\"X_test  shape: {X_test.shape}\")\n",
    "print(y_train.shape)  # Should be (num_samples, num_classes)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Praveen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from tensorflow.keras.layers import LSTM\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Praveen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9729 - loss: 0.2714 \n",
      "Accuracy : 0.9718969464302063\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Confusion Matrix Accuracy: \n",
      " [[143   5   0]\n",
      " [  0 131   5]\n",
      " [  0   2 141]]\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=input_shape),\n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(y_train.shape[1], activation='softmax') \n",
    "    ])\n",
    "    return model\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = get_cnn_model(input_shape)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), verbose=0)\n",
    "model.save(\"../../Models/EEG/cnnmodel.h5\")\n",
    "score, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Accuracy : {acc}\")\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "predict_classes = np.argmax(pred, axis=1)\n",
    "expected_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "conf_matrix = confusion_matrix(expected_classes, predict_classes)\n",
    "print(\"Confusion Matrix Accuracy: \\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Praveen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9807 - loss: 0.0702 \n",
      "Accuracy after Fine tuning Model 0.9812646508216858\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "Confusion Matrix Accuracy: \n",
      " [[148   0   0]\n",
      " [  1 129   6]\n",
      " [  0   1 142]]\n"
     ]
    }
   ],
   "source": [
    "def get_LSTM_model():\n",
    "    model = Sequential([\n",
    "        LSTM(64, input_shape=(1,2548),activation=\"relu\",return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32,activation=\"relu\",return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(16,activation=\"relu\"),\n",
    "        Dropout(0.2),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = get_LSTM_model()\n",
    "\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs = 20, validation_data= (X_test, y_test), verbose = 0)\n",
    "model.save(\"../../Models/EEG/lstmmodel.h5\")\n",
    "score, acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Accuracy after Fine tuning Model {acc}\")\n",
    "\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "predict_classes = np.argmax(pred,axis=1)\n",
    "expected_classes = np.argmax(y_test,axis=1)\n",
    "\n",
    "conf_matrix = confusion_matrix(expected_classes,predict_classes)\n",
    "print(\"Confusion Matrix Accuracy: \\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM: 0.9578454332552693\n",
      "Confusion Matrix of SVM:\n",
      " [[284   7]\n",
      " [ 11 125]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=1,probability=True)\n",
    "svm_model.fit(X_train.reshape(X_train.shape[0], -1), np.argmax(y_train, axis=1))\n",
    "joblib.dump(svm_model,\"../../Models/EEG/svmmodel.joblib\")\n",
    "y_pred_svm = svm_model.predict(X_test.reshape(X_test.shape[0], -1))\n",
    "\n",
    "accuracy_svm = accuracy_score(np.argmax(y_test, axis=1), y_pred_svm)\n",
    "conf_matrix_svm = confusion_matrix(np.argmax(y_test, axis=1), y_pred_svm)\n",
    "\n",
    "print(f\"Accuracy of SVM: {accuracy_svm}\")\n",
    "print(\"Confusion Matrix of SVM:\\n\", conf_matrix_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree: 0.9391100702576113\n",
      "Confusion Matrix of Decision Tree:\n",
      " [[277  14]\n",
      " [ 12 124]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "decision_tree_model = DecisionTreeClassifier()\n",
    "decision_tree_model.fit(X_train.reshape(X_train.shape[0], -1), np.argmax(y_train, axis=1))\n",
    "joblib.dump(decision_tree_model,\"../../Models/EEG/dtmodel.joblib\")\n",
    "y_pred_dt = decision_tree_model.predict(X_test.reshape(X_test.shape[0], -1))\n",
    "\n",
    "accuracy_dt = accuracy_score(np.argmax(y_test, axis=1), y_pred_dt)\n",
    "conf_matrix_dt = confusion_matrix(np.argmax(y_test, axis=1), y_pred_dt)\n",
    "\n",
    "print(f\"Accuracy of Decision Tree: {accuracy_dt}\")\n",
    "print(\"Confusion Matrix of Decision Tree:\\n\", conf_matrix_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest: 0.9953161592505855\n",
      "Confusion Matrix of Random Forest:\n",
      " [[291   0]\n",
      " [  2 134]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "random_forest_model.fit(X_train.reshape(X_train.shape[0], -1), np.argmax(y_train, axis=1))\n",
    "\n",
    "y_pred_rf = random_forest_model.predict(X_test.reshape(X_test.shape[0], -1))\n",
    "\n",
    "accuracy_rf = accuracy_score(np.argmax(y_test, axis=1), y_pred_rf)\n",
    "conf_matrix_rf = confusion_matrix(np.argmax(y_test, axis=1), y_pred_rf)\n",
    "\n",
    "print(f\"Accuracy of Random Forest: {accuracy_rf}\")\n",
    "print(\"Confusion Matrix of Random Forest:\\n\", conf_matrix_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - loss: 1.3141 - val_loss: 0.8708\n",
      "Epoch 2/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 1.1872 - val_loss: 0.8255\n",
      "Epoch 3/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.8246 - val_loss: 0.7939\n",
      "Epoch 4/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.8911 - val_loss: 0.7487\n",
      "Epoch 5/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.8488 - val_loss: 0.7137\n",
      "Epoch 6/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.8227 - val_loss: 0.6806\n",
      "Epoch 7/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.8925 - val_loss: 0.6608\n",
      "Epoch 8/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.8269 - val_loss: 0.6470\n",
      "Epoch 9/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.8743 - val_loss: 0.6473\n",
      "Epoch 10/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.7754 - val_loss: 0.6413\n",
      "Epoch 11/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.8562 - val_loss: 0.6298\n",
      "Epoch 12/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.7926 - val_loss: 0.6244\n",
      "Epoch 13/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.6828 - val_loss: 0.6135\n",
      "Epoch 14/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.7867 - val_loss: 0.6200\n",
      "Epoch 15/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.8214 - val_loss: 0.6095\n",
      "Epoch 16/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.7450 - val_loss: 0.6065\n",
      "Epoch 17/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 1.0071 - val_loss: 0.6042\n",
      "Epoch 18/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 1.0336 - val_loss: 0.6008\n",
      "Epoch 19/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.8899 - val_loss: 0.5998\n",
      "Epoch 20/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.8661 - val_loss: 0.5977\n",
      "Epoch 21/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.7441 - val_loss: 0.5951\n",
      "Epoch 22/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.8040 - val_loss: 0.5951\n",
      "Epoch 23/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.9523 - val_loss: 0.5930\n",
      "Epoch 24/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.7987 - val_loss: 0.5951\n",
      "Epoch 25/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.7083 - val_loss: 0.5960\n",
      "Epoch 26/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.7971 - val_loss: 0.5923\n",
      "Epoch 27/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.7411 - val_loss: 0.5936\n",
      "Epoch 28/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.7561 - val_loss: 0.5927\n",
      "Epoch 29/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.6516 - val_loss: 0.5956\n",
      "Epoch 30/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.6895 - val_loss: 0.5926\n",
      "Epoch 31/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.6955 - val_loss: 0.5918\n",
      "Epoch 32/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.8820 - val_loss: 0.5899\n",
      "Epoch 33/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.8791 - val_loss: 0.5896\n",
      "Epoch 34/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.7348 - val_loss: 0.5893\n",
      "Epoch 35/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.8583 - val_loss: 0.5886\n",
      "Epoch 36/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.7995 - val_loss: 0.5884\n",
      "Epoch 37/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.7024 - val_loss: 0.5883\n",
      "Epoch 38/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.8515 - val_loss: 0.5887\n",
      "Epoch 39/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.7665 - val_loss: 0.5879\n",
      "Epoch 40/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.7514 - val_loss: 0.5875\n",
      "Epoch 41/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.6588 - val_loss: 0.5871\n",
      "Epoch 42/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.7173 - val_loss: 0.5871\n",
      "Epoch 43/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.7185 - val_loss: 0.5877\n",
      "Epoch 44/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.8840 - val_loss: 0.5861\n",
      "Epoch 45/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.7587 - val_loss: 0.5860\n",
      "Epoch 46/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.8679 - val_loss: 0.5865\n",
      "Epoch 47/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.8145 - val_loss: 0.5861\n",
      "Epoch 48/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.8814 - val_loss: 0.5849\n",
      "Epoch 49/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.7845 - val_loss: 0.5854\n",
      "Epoch 50/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.8569 - val_loss: 0.5849\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Praveen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.3973 - loss: 1.8944 - val_accuracy: 0.4801 - val_loss: 0.8516\n",
      "Epoch 2/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6215 - loss: 1.0085 - val_accuracy: 0.8642 - val_loss: 0.5028\n",
      "Epoch 3/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7488 - loss: 0.7180 - val_accuracy: 0.8946 - val_loss: 0.3775\n",
      "Epoch 4/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7571 - loss: 0.6745 - val_accuracy: 0.8946 - val_loss: 0.3314\n",
      "Epoch 5/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8036 - loss: 0.5657 - val_accuracy: 0.8899 - val_loss: 0.3054\n",
      "Epoch 6/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8111 - loss: 0.4978 - val_accuracy: 0.9063 - val_loss: 0.2791\n",
      "Epoch 7/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8425 - loss: 0.4997 - val_accuracy: 0.9087 - val_loss: 0.2573\n",
      "Epoch 8/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8639 - loss: 0.4365 - val_accuracy: 0.9133 - val_loss: 0.2513\n",
      "Epoch 9/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8404 - loss: 0.4669 - val_accuracy: 0.9110 - val_loss: 0.2533\n",
      "Epoch 10/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8388 - loss: 0.4171 - val_accuracy: 0.9087 - val_loss: 0.2399\n",
      "Epoch 11/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8511 - loss: 0.4289 - val_accuracy: 0.9133 - val_loss: 0.2330\n",
      "Epoch 12/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8686 - loss: 0.3726 - val_accuracy: 0.9110 - val_loss: 0.2394\n",
      "Epoch 13/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8556 - loss: 0.4316 - val_accuracy: 0.9133 - val_loss: 0.2222\n",
      "Epoch 14/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8813 - loss: 0.3627 - val_accuracy: 0.9157 - val_loss: 0.2158\n",
      "Epoch 15/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8708 - loss: 0.3557 - val_accuracy: 0.9133 - val_loss: 0.2185\n",
      "Epoch 16/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8770 - loss: 0.3201 - val_accuracy: 0.9157 - val_loss: 0.2157\n",
      "Epoch 17/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8905 - loss: 0.3099 - val_accuracy: 0.9110 - val_loss: 0.2237\n",
      "Epoch 18/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8713 - loss: 0.3329 - val_accuracy: 0.9157 - val_loss: 0.2086\n",
      "Epoch 19/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8960 - loss: 0.3011 - val_accuracy: 0.9133 - val_loss: 0.2138\n",
      "Epoch 20/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8758 - loss: 0.3079 - val_accuracy: 0.9133 - val_loss: 0.2090\n",
      "Epoch 21/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8651 - loss: 0.3513 - val_accuracy: 0.9133 - val_loss: 0.2072\n",
      "Epoch 22/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9131 - loss: 0.2746 - val_accuracy: 0.9180 - val_loss: 0.1968\n",
      "Epoch 23/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8892 - loss: 0.2958 - val_accuracy: 0.9133 - val_loss: 0.2021\n",
      "Epoch 24/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8842 - loss: 0.2869 - val_accuracy: 0.9180 - val_loss: 0.1940\n",
      "Epoch 25/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8858 - loss: 0.3314 - val_accuracy: 0.9157 - val_loss: 0.1987\n",
      "Epoch 26/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9148 - loss: 0.2439 - val_accuracy: 0.9180 - val_loss: 0.1943\n",
      "Epoch 27/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9068 - loss: 0.2804 - val_accuracy: 0.9180 - val_loss: 0.1990\n",
      "Epoch 28/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8880 - loss: 0.2828 - val_accuracy: 0.9133 - val_loss: 0.2039\n",
      "Epoch 29/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8985 - loss: 0.2749 - val_accuracy: 0.9063 - val_loss: 0.2099\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9253 - loss: 0.1895 \n",
      "Classifier Accuracy: 0.9180327653884888\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Confusion Matrix:\n",
      " [[144   4   0]\n",
      " [  2 105  29]\n",
      " [  0   0 143]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, MaxPooling1D, UpSampling1D, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def add_noise(data, noise_factor=0.2):  \n",
    "    noisy_data = data + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=data.shape)\n",
    "    return np.clip(noisy_data, 0., 1.)\n",
    "\n",
    "def get_denoising_autoencoder(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    \n",
    "    x = Conv1D(64, 3, activation='relu', padding='same')(input_layer)\n",
    "    x = BatchNormalization()(x)  \n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Conv1D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    encoded = MaxPooling1D(2, padding='same')(x)\n",
    "\n",
    "    x = Conv1D(128, 3, activation='relu', padding='same')(encoded)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = UpSampling1D(2)(x)\n",
    "    x = Conv1D(64, 3, activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = UpSampling1D(2)(x)\n",
    "    decoded = Conv1D(input_shape[-1], 3, activation='sigmoid', padding='same')(x)\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    return autoencoder\n",
    "\n",
    "def get_classifier_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=input_shape),\n",
    "        Dense(128, activation='relu'),  \n",
    "        BatchNormalization(),  \n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),  \n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(3, activation='softmax')  \n",
    "    ])\n",
    "    return model\n",
    "\n",
    "X_train_noisy = add_noise(X_train)\n",
    "X_test_noisy = add_noise(X_test)\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "autoencoder = get_denoising_autoencoder(input_shape)\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')  \n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "autoencoder.fit(X_train_noisy, X_train, epochs=50, batch_size=64, validation_data=(X_test_noisy, X_test), \n",
    "                callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "encoder = Model(inputs=autoencoder.input, outputs=autoencoder.layers[4].output)  \n",
    "\n",
    "X_train_encoded = encoder.predict(X_train)\n",
    "X_test_encoded = encoder.predict(X_test)\n",
    "\n",
    "classifier = get_classifier_model(X_train_encoded.shape[1:])\n",
    "classifier.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "classifier.fit(X_train_encoded, y_train, epochs=50, batch_size=64, validation_data=(X_test_encoded, y_test), \n",
    "               callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "score, acc = classifier.evaluate(X_test_encoded, y_test)\n",
    "print(f\"Classifier Accuracy: {acc}\")\n",
    "\n",
    "y_pred = classifier.predict(X_test_encoded)\n",
    "predict_classes = np.argmax(y_pred, axis=1)\n",
    "expected_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "conf_matrix = confusion_matrix(expected_classes, predict_classes)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Praveen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.4807 - loss: 1.9280 - val_accuracy: 0.7541 - val_loss: 1.7159\n",
      "Epoch 2/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7021 - loss: 1.3169 - val_accuracy: 0.8946 - val_loss: 1.0928\n",
      "Epoch 3/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8233 - loss: 1.0900 - val_accuracy: 0.9040 - val_loss: 0.9169\n",
      "Epoch 4/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8265 - loss: 1.0476 - val_accuracy: 0.9133 - val_loss: 0.8209\n",
      "Epoch 5/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8538 - loss: 0.9802 - val_accuracy: 0.9157 - val_loss: 0.7830\n",
      "Epoch 6/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8739 - loss: 0.8856 - val_accuracy: 0.9180 - val_loss: 0.7782\n",
      "Epoch 7/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8527 - loss: 0.9212 - val_accuracy: 0.9204 - val_loss: 0.7607\n",
      "Epoch 8/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8637 - loss: 0.9123 - val_accuracy: 0.9133 - val_loss: 0.7405\n",
      "Epoch 9/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8791 - loss: 0.8563 - val_accuracy: 0.9180 - val_loss: 0.7169\n",
      "Epoch 10/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8840 - loss: 0.8394 - val_accuracy: 0.9204 - val_loss: 0.6978\n",
      "Epoch 11/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8785 - loss: 0.8362 - val_accuracy: 0.9180 - val_loss: 0.7008\n",
      "Epoch 12/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8823 - loss: 0.7878 - val_accuracy: 0.9344 - val_loss: 0.6814\n",
      "Epoch 13/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8823 - loss: 0.8229 - val_accuracy: 0.9180 - val_loss: 0.6752\n",
      "Epoch 14/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8844 - loss: 0.7852 - val_accuracy: 0.9297 - val_loss: 0.6751\n",
      "Epoch 15/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8882 - loss: 0.7708 - val_accuracy: 0.9297 - val_loss: 0.6637\n",
      "Epoch 16/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8926 - loss: 0.7615 - val_accuracy: 0.9344 - val_loss: 0.6496\n",
      "Epoch 17/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8973 - loss: 0.7499 - val_accuracy: 0.9297 - val_loss: 0.6434\n",
      "Epoch 18/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8895 - loss: 0.7725 - val_accuracy: 0.9438 - val_loss: 0.6389\n",
      "Epoch 19/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8859 - loss: 0.7306 - val_accuracy: 0.9461 - val_loss: 0.6321\n",
      "Epoch 20/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8941 - loss: 0.7039 - val_accuracy: 0.9297 - val_loss: 0.6342\n",
      "Epoch 21/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9006 - loss: 0.6806 - val_accuracy: 0.9368 - val_loss: 0.6083\n",
      "Epoch 22/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8891 - loss: 0.7153 - val_accuracy: 0.9391 - val_loss: 0.6048\n",
      "Epoch 23/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8884 - loss: 0.6962 - val_accuracy: 0.9344 - val_loss: 0.5984\n",
      "Epoch 24/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8910 - loss: 0.6833 - val_accuracy: 0.9368 - val_loss: 0.5870\n",
      "Epoch 25/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8853 - loss: 0.6632 - val_accuracy: 0.9368 - val_loss: 0.5791\n",
      "Epoch 26/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9168 - loss: 0.6213 - val_accuracy: 0.9415 - val_loss: 0.5820\n",
      "Epoch 27/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9082 - loss: 0.6199 - val_accuracy: 0.9274 - val_loss: 0.5803\n",
      "Epoch 28/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8903 - loss: 0.6510 - val_accuracy: 0.9344 - val_loss: 0.5551\n",
      "Epoch 29/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9164 - loss: 0.6114 - val_accuracy: 0.9391 - val_loss: 0.5503\n",
      "Epoch 30/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9091 - loss: 0.5972 - val_accuracy: 0.9227 - val_loss: 0.5567\n",
      "Epoch 31/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8944 - loss: 0.6355 - val_accuracy: 0.9344 - val_loss: 0.5491\n",
      "Epoch 32/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9153 - loss: 0.5657 - val_accuracy: 0.9461 - val_loss: 0.5276\n",
      "Epoch 33/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9146 - loss: 0.5674 - val_accuracy: 0.9321 - val_loss: 0.5281\n",
      "Epoch 34/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9096 - loss: 0.5754 - val_accuracy: 0.9297 - val_loss: 0.5237\n",
      "Epoch 35/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9037 - loss: 0.5550 - val_accuracy: 0.9415 - val_loss: 0.5132\n",
      "Epoch 36/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9008 - loss: 0.5790 - val_accuracy: 0.9461 - val_loss: 0.4994\n",
      "Epoch 37/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9089 - loss: 0.5675 - val_accuracy: 0.9321 - val_loss: 0.5081\n",
      "Epoch 38/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8996 - loss: 0.5509 - val_accuracy: 0.9368 - val_loss: 0.4941\n",
      "Epoch 39/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9149 - loss: 0.5462 - val_accuracy: 0.9391 - val_loss: 0.4857\n",
      "Epoch 40/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9162 - loss: 0.5358 - val_accuracy: 0.9274 - val_loss: 0.4828\n",
      "Epoch 41/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9155 - loss: 0.5034 - val_accuracy: 0.9368 - val_loss: 0.4927\n",
      "Epoch 42/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9061 - loss: 0.5459 - val_accuracy: 0.9368 - val_loss: 0.4782\n",
      "Epoch 43/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9016 - loss: 0.5493 - val_accuracy: 0.9415 - val_loss: 0.4679\n",
      "Epoch 44/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9201 - loss: 0.4995 - val_accuracy: 0.9391 - val_loss: 0.4600\n",
      "Epoch 45/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9165 - loss: 0.5064 - val_accuracy: 0.9368 - val_loss: 0.4549\n",
      "Epoch 46/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9157 - loss: 0.4910 - val_accuracy: 0.9415 - val_loss: 0.4458\n",
      "Epoch 47/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9195 - loss: 0.5079 - val_accuracy: 0.9461 - val_loss: 0.4434\n",
      "Epoch 48/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9133 - loss: 0.4938 - val_accuracy: 0.9415 - val_loss: 0.4387\n",
      "Epoch 49/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9209 - loss: 0.4676 - val_accuracy: 0.9438 - val_loss: 0.4224\n",
      "Epoch 50/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9187 - loss: 0.4527 - val_accuracy: 0.9321 - val_loss: 0.4263\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9354 - loss: 0.4294 \n",
      "Classifier Accuracy: 0.9437938928604126\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Confusion Matrix:\n",
      " [[140   8   0]\n",
      " [  1 122  13]\n",
      " [  0   2 141]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import LeakyReLU, Add, Input, Dense, Dropout, BatchNormalization, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def get_complex_classifier_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Flatten()(inputs)\n",
    "    x = Dense(256, kernel_regularizer=l2(0.001))(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x1 = Dense(128, kernel_regularizer=l2(0.001))(x)\n",
    "    x1 = LeakyReLU(alpha=0.1)(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x2 = Dense(128, kernel_regularizer=l2(0.001))(x1)\n",
    "    x2 = LeakyReLU(alpha=0.1)(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "\n",
    "    x = Add()([x1, x2])\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Dense(64, kernel_regularizer=l2(0.001))(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    outputs = Dense(3, activation='softmax')(x)  \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "complex_classifier = get_complex_classifier_model(X_train_encoded.shape[1:])\n",
    "complex_classifier.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "complex_classifier.fit(X_train_encoded, y_train, epochs=50, batch_size=64, validation_data=(X_test_encoded, y_test), \n",
    "                       callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "score, acc = complex_classifier.evaluate(X_test_encoded, y_test)\n",
    "print(f\"Classifier Accuracy: {acc}\")\n",
    "\n",
    "y_pred = complex_classifier.predict(X_test_encoded)\n",
    "predict_classes = np.argmax(y_pred, axis=1)\n",
    "expected_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "conf_matrix = confusion_matrix(expected_classes, predict_classes)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
