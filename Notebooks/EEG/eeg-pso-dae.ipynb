{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9598491,"sourceType":"datasetVersion","datasetId":5855338}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyswarm","metadata":{"execution":{"iopub.status.busy":"2024-10-22T10:08:49.715380Z","iopub.execute_input":"2024-10-22T10:08:49.715767Z","iopub.status.idle":"2024-10-22T10:09:03.502964Z","shell.execute_reply.started":"2024-10-22T10:08:49.715727Z","shell.execute_reply":"2024-10-22T10:09:03.501811Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nimport pandas as pd\n\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.utils import to_categorical\n\nimport joblib\n\nimport tensorflow as tf\n\nfrom tensorflow.keras.models import Sequential\n\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n\nfrom tensorflow.keras.utils import to_categorical\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\ntf.keras.backend.clear_session()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T10:09:03.505102Z","iopub.execute_input":"2024-10-22T10:09:03.505445Z","iopub.status.idle":"2024-10-22T10:09:03.744979Z","shell.execute_reply.started":"2024-10-22T10:09:03.505409Z","shell.execute_reply":"2024-10-22T10:09:03.744029Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/eeg-data/preprocessed_dataset.csv')\n\nsns.countplot(x='label', data=df)\n\n\n\n# df.isnull().sum().sum()\n\n\n\nencode = ({'NEUTRAL': 0, 'POSITIVE': 1, 'NEGATIVE': 0} )\n\ndf_encoded = df.replace(encode)\n\n\n\nX=df_encoded.drop([\"label\"]  ,axis=1)\n\ny = df_encoded.loc[:,'label'].values\n\nX_orig=df_encoded.drop([\"label\"]  ,axis=1)\n\ny_orig = to_categorical(df_encoded.loc[:,'label'].values)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T10:09:03.746520Z","iopub.execute_input":"2024-10-22T10:09:03.746851Z","iopub.status.idle":"2024-10-22T10:09:05.283906Z","shell.execute_reply.started":"2024-10-22T10:09:03.746802Z","shell.execute_reply":"2024-10-22T10:09:05.283087Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaler = StandardScaler()\n\nscaler.fit(X)\n\nX = scaler.transform(X)\n\ny = to_categorical(y)\n\n# print(y.shape)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\nprint(X_train.shape)\n\nX_train = np.reshape(X_train, (X_train.shape[0],1,X.shape[1]))\n\nX_test = np.reshape(X_test, (X_test.shape[0],1,X.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2024-10-22T10:09:05.285798Z","iopub.execute_input":"2024-10-22T10:09:05.286088Z","iopub.status.idle":"2024-10-22T10:09:05.497589Z","shell.execute_reply.started":"2024-10-22T10:09:05.286056Z","shell.execute_reply":"2024-10-22T10:09:05.496642Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nimport tensorflow as tf\n\nfrom tensorflow.keras.models import Model, Sequential\n\nfrom tensorflow.keras.layers import Input, Dense, Conv1D, MaxPooling1D, UpSampling1D, Flatten, Dropout, BatchNormalization\n\nfrom tensorflow.keras.optimizers import Adam\n\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nfrom sklearn.metrics import confusion_matrix\n\nfrom pyswarm import pso  # Install pyswarm for PSO algorithm\n\n\n\n# Helper function to add noise\n\ndef add_noise(data, noise_factor=0.2):  \n\n    noisy_data = data + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=data.shape)\n\n    return np.clip(noisy_data, 0., 1.)\n\n\n\n# Denoising Autoencoder architecture\n\ndef get_denoising_autoencoder(input_shape):\n\n    input_layer = Input(shape=input_shape)\n\n    \n\n    x = Conv1D(64, 2, activation='relu', padding='same')(input_layer)\n\n    x = BatchNormalization()(x)  \n\n    x = MaxPooling1D(2, padding='same')(x)\n\n    x = Conv1D(128, 2, activation='relu', padding='same')(x)\n\n    x = BatchNormalization()(x)\n\n    encoded = MaxPooling1D(2, padding='same')(x)\n\n\n\n    x = Conv1D(128, 2, activation='relu', padding='same')(encoded)\n\n    x = BatchNormalization()(x)\n\n    x = UpSampling1D(2)(x)\n\n    x = Conv1D(64, 2, activation='relu', padding='same')(x)\n\n    x = BatchNormalization()(x)\n\n    x = UpSampling1D(2)(x)\n\n    decoded = Conv1D(input_shape[-1], 2, activation='sigmoid', padding='same')(x)\n\n    autoencoder = Model(input_layer, decoded)\n\n    return autoencoder\n\n\n\n# Classifier model\n\ndef get_classifier_model(input_shape, dense_units_1=128, dense_units_2=64, dense_units_3=32, dropout_rate=0.5):\n\n    model = Sequential([\n\n        Flatten(input_shape=input_shape),\n\n        Dense(dense_units_1, activation='relu'),  \n\n        BatchNormalization(),  \n\n        Dropout(dropout_rate),\n\n        Dense(dense_units_2, activation='relu'),\n\n        BatchNormalization(),  \n\n        Dropout(dropout_rate),\n\n        Dense(dense_units_3, activation='relu'),\n\n        Dropout(dropout_rate),\n\n        Dense(2, activation='sigmoid')  # Assuming 3 classes\n\n    ])\n\n    return model\n\n\n\n# PSO objective function: optimize learning_rate and number of units in dense layers\n\ndef pso_objective(params):\n\n    learning_rate, dense_units_1, dense_units_2, dense_units_3, dropout_rate = params\n\n    \n\n    autoencoder = get_denoising_autoencoder(input_shape)\n\n    autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')  # Autoencoder not affected by PSO\n\n    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n\n\n    # Train autoencoder\n\n    autoencoder.fit(X_train_noisy, X_train, epochs=10, batch_size=64, validation_data=(X_test_noisy, X_test), \n\n                    callbacks=[early_stopping], verbose=0)\n\n\n\n    encoder = Model(inputs=autoencoder.input, outputs=autoencoder.layers[4].output)\n\n    X_train_encoded = encoder.predict(X_train)\n\n    X_test_encoded = encoder.predict(X_test)\n\n\n\n    classifier = get_classifier_model(X_train_encoded.shape[1:], dense_units_1=int(dense_units_1), \n\n                                      dense_units_2=int(dense_units_2), dense_units_3=int(dense_units_3), \n\n                                      dropout_rate=dropout_rate)\n\n    classifier.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n\n\n\n    history = classifier.fit(X_train_encoded, y_train, epochs=10, batch_size=64, validation_data=(X_test_encoded, y_test), \n\n                             callbacks=[early_stopping], verbose=0)\n\n\n\n    # Return negative of validation accuracy (PSO minimizes, we want to maximize accuracy)\n\n    val_acc = history.history['val_accuracy'][-1]\n\n    return -val_acc\n\n\n\n# PSO bounds: [learning_rate, dense_units_1, dense_units_2, dense_units_3, dropout_rate]\n\nlb = [1e-3, 128, 64, 32, 0.3]  # Lower bounds\n\nub = [1e-2, 130, 70, 40, 0.5]  # Upper bounds\n\n\n\n# Add noise to the training and testing datasets\n\nX_train_noisy = add_noise(X_train)\n\nX_test_noisy = add_noise(X_test)\n\n\n\n# Define input shape from the dataset\n\ninput_shape = (X_train.shape[1], X_train.shape[2])\n\n\n\n# Perform PSO\n\nbest_params, best_val = pso(pso_objective, lb, ub, swarmsize=20, maxiter=10)\n\n\n\n# Extract best hyperparameters\n\nbest_learning_rate, best_dense_units_1, best_dense_units_2, best_dense_units_3, best_dropout_rate = best_params\n\n\n\nprint(f\"Best Hyperparameters:\\nLearning Rate: {best_learning_rate}\\nDense Units 1: {best_dense_units_1}\\nDense Units 2: {best_dense_units_2}\\nDense Units 3: {best_dense_units_3}\\nDropout Rate: {best_dropout_rate}\")\n\n\n\n# Retrain the final model with optimal hyperparameters\n\nautoencoder = get_denoising_autoencoder(input_shape)\n\nautoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n\n\nautoencoder.fit(X_train_noisy, X_train, epochs=50, batch_size=64, validation_data=(X_test_noisy, X_test), \n\n                callbacks=[early_stopping], verbose=1)\n\n\n\nencoder = Model(inputs=autoencoder.input, outputs=autoencoder.layers[4].output)\n\nX_train_encoded = encoder.predict(X_train)\n\nX_test_encoded = encoder.predict(X_test)\n\n\n\nclassifier = get_classifier_model(X_train_encoded.shape[1:], dense_units_1=int(best_dense_units_1), \n\n                                  dense_units_2=int(best_dense_units_2), dense_units_3=int(best_dense_units_3), \n\n                                  dropout_rate=best_dropout_rate)\n\nclassifier.compile(optimizer=Adam(learning_rate=best_learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n\n\n\nclassifier.fit(X_train_encoded, y_train, epochs=50, batch_size=64, validation_data=(X_test_encoded, y_test), \n\n               callbacks=[early_stopping], verbose=1)\n\n\n\n# Evaluate the final model\n\nscore, acc = classifier.evaluate(X_test_encoded, y_test)\n\nprint(f\"Final Classifier Accuracy: {acc}\")\n\n\n\n# Save the best model\n\nclassifier.save('best_classifier_model.h5')\n\n\n\n# Print confusion matrix\n\ny_pred = classifier.predict(X_test_encoded)\n\npredict_classes = np.argmax(y_pred, axis=1)\n\nexpected_classes = np.argmax(y_test, axis=1)\n\nconf_matrix = confusion_matrix(expected_classes, predict_classes)\n\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T10:09:05.498910Z","iopub.execute_input":"2024-10-22T10:09:05.499278Z","iopub.status.idle":"2024-10-22T10:40:39.704559Z","shell.execute_reply.started":"2024-10-22T10:09:05.499243Z","shell.execute_reply":"2024-10-22T10:40:39.703275Z"},"trusted":true},"outputs":[],"execution_count":null}]}