{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import joblib\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># mean_0_a</th>\n",
       "      <th>mean_1_a</th>\n",
       "      <th>mean_2_a</th>\n",
       "      <th>mean_3_a</th>\n",
       "      <th>mean_4_a</th>\n",
       "      <th>mean_d_0_a</th>\n",
       "      <th>mean_d_1_a</th>\n",
       "      <th>mean_d_2_a</th>\n",
       "      <th>mean_d_3_a</th>\n",
       "      <th>mean_d_4_a</th>\n",
       "      <th>...</th>\n",
       "      <th>fft_741_b</th>\n",
       "      <th>fft_742_b</th>\n",
       "      <th>fft_743_b</th>\n",
       "      <th>fft_744_b</th>\n",
       "      <th>fft_745_b</th>\n",
       "      <th>fft_746_b</th>\n",
       "      <th>fft_747_b</th>\n",
       "      <th>fft_748_b</th>\n",
       "      <th>fft_749_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.62</td>\n",
       "      <td>30.3</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>26.3</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-15.70</td>\n",
       "      <td>2.06</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>23.5</td>\n",
       "      <td>20.3</td>\n",
       "      <td>20.3</td>\n",
       "      <td>23.5</td>\n",
       "      <td>-215.0</td>\n",
       "      <td>280.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>280.00</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.80</td>\n",
       "      <td>33.1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>22.8</td>\n",
       "      <td>6.550</td>\n",
       "      <td>1.680</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.83</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>182.0</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>2.57</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.90</td>\n",
       "      <td>29.4</td>\n",
       "      <td>-416.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>23.7</td>\n",
       "      <td>79.900</td>\n",
       "      <td>3.360</td>\n",
       "      <td>90.20</td>\n",
       "      <td>89.90</td>\n",
       "      <td>2.03</td>\n",
       "      <td>...</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-267.0</td>\n",
       "      <td>281.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>281.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.90</td>\n",
       "      <td>31.6</td>\n",
       "      <td>-143.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>24.3</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>8.82</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>...</td>\n",
       "      <td>299.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>9.53</td>\n",
       "      <td>9.53</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.30</td>\n",
       "      <td>31.3</td>\n",
       "      <td>45.2</td>\n",
       "      <td>27.3</td>\n",
       "      <td>24.5</td>\n",
       "      <td>34.800</td>\n",
       "      <td>-5.790</td>\n",
       "      <td>3.06</td>\n",
       "      <td>41.40</td>\n",
       "      <td>5.52</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.1</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.90</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  mean_d_1_a  \\\n",
       "0        4.62      30.3    -356.0      15.6      26.3       1.070       0.411   \n",
       "1       28.80      33.1      32.0      25.8      22.8       6.550       1.680   \n",
       "2        8.90      29.4    -416.0      16.7      23.7      79.900       3.360   \n",
       "3       14.90      31.6    -143.0      19.8      24.3      -0.584      -0.284   \n",
       "4       28.30      31.3      45.2      27.3      24.5      34.800      -5.790   \n",
       "\n",
       "   mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  fft_742_b  fft_743_b  \\\n",
       "0      -15.70        2.06        3.15  ...       23.5       20.3       20.3   \n",
       "1        2.88        3.83       -4.82  ...      -23.3      -21.8      -21.8   \n",
       "2       90.20       89.90        2.03  ...      462.0     -233.0     -233.0   \n",
       "3        8.82        2.30       -1.97  ...      299.0     -243.0     -243.0   \n",
       "4        3.06       41.40        5.52  ...       12.0       38.1       38.1   \n",
       "\n",
       "   fft_744_b  fft_745_b  fft_746_b  fft_747_b  fft_748_b  fft_749_b     label  \n",
       "0       23.5     -215.0     280.00    -162.00    -162.00     280.00  NEGATIVE  \n",
       "1      -23.3      182.0       2.57     -31.60     -31.60       2.57   NEUTRAL  \n",
       "2      462.0     -267.0     281.00    -148.00    -148.00     281.00  POSITIVE  \n",
       "3      299.0      132.0     -12.40       9.53       9.53     -12.40  POSITIVE  \n",
       "4       12.0      119.0     -17.60      23.90      23.90     -17.60   NEUTRAL  \n",
       "\n",
       "[5 rows x 2549 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('preprocessed_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['# mean_0_a', 'mean_1_a', 'mean_2_a', 'mean_3_a', 'mean_4_a',\n",
       "       'mean_d_0_a', 'mean_d_1_a', 'mean_d_2_a', 'mean_d_3_a', 'mean_d_4_a',\n",
       "       ...\n",
       "       'fft_741_b', 'fft_742_b', 'fft_743_b', 'fft_744_b', 'fft_745_b',\n",
       "       'fft_746_b', 'fft_747_b', 'fft_748_b', 'fft_749_b', 'label'],\n",
       "      dtype='object', length=2549)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Praveen\\AppData\\Local\\Temp\\ipykernel_12848\\3020761041.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_encoded = df.replace(encode)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encode = ({'NEUTRAL': 0, 'POSITIVE': 1, 'NEGATIVE': 2} )\n",
    "df_encoded = df.replace(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df_encoded.corr()\n",
    "target_correlation = correlation_matrix['label'].abs()\n",
    "sorted_correlation = target_correlation.sort_values(ascending=False)\n",
    "top_75_features = sorted_correlation.iloc[1:76].index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train  shape: (1705, 1, 75)\n",
      "X_test  shape: (427, 1, 75)\n",
      "(1705, 3)\n",
      "(427, 3)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create X and y\n",
    "X = df_encoded[top_75_features]  # Selecting the top 75 features\n",
    "y = df_encoded['label'] \n",
    "y = to_categorical(y)         # Target column\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "# Step 3: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0],1,X.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0],1,X.shape[1]))\n",
    "print(f\"X_train  shape: {X_train.shape}\")\n",
    "print(f\"X_test  shape: {X_test.shape}\")\n",
    "print(y_train.shape)  # Should be (num_samples, num_classes)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_2_a</th>\n",
       "      <th>min_2_b</th>\n",
       "      <th>min_q_7_a</th>\n",
       "      <th>min_q_17_a</th>\n",
       "      <th>min_q_7_b</th>\n",
       "      <th>min_q_17_b</th>\n",
       "      <th>min_q_2_a</th>\n",
       "      <th>min_q_2_b</th>\n",
       "      <th>min_q_12_a</th>\n",
       "      <th>min_q_12_b</th>\n",
       "      <th>...</th>\n",
       "      <th>fft_409_a</th>\n",
       "      <th>fft_1_b</th>\n",
       "      <th>fft_4_b</th>\n",
       "      <th>fft_4_a</th>\n",
       "      <th>fft_1_a</th>\n",
       "      <th>fft_409_b</th>\n",
       "      <th>fft_406_b</th>\n",
       "      <th>fft_360_a</th>\n",
       "      <th>fft_315_a</th>\n",
       "      <th>fft_690_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-727.000</td>\n",
       "      <td>-727.000</td>\n",
       "      <td>-676.000</td>\n",
       "      <td>-727.00</td>\n",
       "      <td>-727.00</td>\n",
       "      <td>-658.000</td>\n",
       "      <td>-677.00</td>\n",
       "      <td>-702.00</td>\n",
       "      <td>-698.00</td>\n",
       "      <td>-700.00</td>\n",
       "      <td>...</td>\n",
       "      <td>445.0</td>\n",
       "      <td>438.00</td>\n",
       "      <td>438.00</td>\n",
       "      <td>398.00</td>\n",
       "      <td>398.00</td>\n",
       "      <td>508.00</td>\n",
       "      <td>508.00</td>\n",
       "      <td>-508.0</td>\n",
       "      <td>-359.0</td>\n",
       "      <td>-462.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.705</td>\n",
       "      <td>12.700</td>\n",
       "      <td>-0.705</td>\n",
       "      <td>12.00</td>\n",
       "      <td>12.70</td>\n",
       "      <td>13.500</td>\n",
       "      <td>11.10</td>\n",
       "      <td>16.00</td>\n",
       "      <td>16.30</td>\n",
       "      <td>17.70</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.2</td>\n",
       "      <td>-4.00</td>\n",
       "      <td>-4.00</td>\n",
       "      <td>-4.30</td>\n",
       "      <td>-4.30</td>\n",
       "      <td>-5.64</td>\n",
       "      <td>-5.64</td>\n",
       "      <td>191.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-844.000</td>\n",
       "      <td>-814.000</td>\n",
       "      <td>-844.000</td>\n",
       "      <td>-822.00</td>\n",
       "      <td>-803.00</td>\n",
       "      <td>-743.000</td>\n",
       "      <td>-707.00</td>\n",
       "      <td>-814.00</td>\n",
       "      <td>-826.00</td>\n",
       "      <td>-610.00</td>\n",
       "      <td>...</td>\n",
       "      <td>282.0</td>\n",
       "      <td>361.00</td>\n",
       "      <td>361.00</td>\n",
       "      <td>204.00</td>\n",
       "      <td>204.00</td>\n",
       "      <td>131.00</td>\n",
       "      <td>131.00</td>\n",
       "      <td>-57.2</td>\n",
       "      <td>-582.0</td>\n",
       "      <td>-125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-819.000</td>\n",
       "      <td>-820.000</td>\n",
       "      <td>-813.000</td>\n",
       "      <td>-712.00</td>\n",
       "      <td>-718.00</td>\n",
       "      <td>-552.000</td>\n",
       "      <td>-671.00</td>\n",
       "      <td>-820.00</td>\n",
       "      <td>-819.00</td>\n",
       "      <td>-584.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-28.4</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.89</td>\n",
       "      <td>6.25</td>\n",
       "      <td>6.25</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.32</td>\n",
       "      <td>217.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9.640</td>\n",
       "      <td>-36.000</td>\n",
       "      <td>-5.480</td>\n",
       "      <td>-9.64</td>\n",
       "      <td>-12.30</td>\n",
       "      <td>7.600</td>\n",
       "      <td>12.20</td>\n",
       "      <td>19.40</td>\n",
       "      <td>20.10</td>\n",
       "      <td>-36.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-55.6</td>\n",
       "      <td>-27.40</td>\n",
       "      <td>-27.40</td>\n",
       "      <td>-2.37</td>\n",
       "      <td>-2.37</td>\n",
       "      <td>56.40</td>\n",
       "      <td>56.40</td>\n",
       "      <td>350.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2127</th>\n",
       "      <td>7.940</td>\n",
       "      <td>-0.538</td>\n",
       "      <td>9.940</td>\n",
       "      <td>8.16</td>\n",
       "      <td>8.89</td>\n",
       "      <td>-0.538</td>\n",
       "      <td>7.94</td>\n",
       "      <td>11.60</td>\n",
       "      <td>11.20</td>\n",
       "      <td>5.95</td>\n",
       "      <td>...</td>\n",
       "      <td>27.3</td>\n",
       "      <td>3.37</td>\n",
       "      <td>3.37</td>\n",
       "      <td>12.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>-4.19</td>\n",
       "      <td>-4.19</td>\n",
       "      <td>142.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>187.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>-756.000</td>\n",
       "      <td>-739.000</td>\n",
       "      <td>-756.000</td>\n",
       "      <td>-674.00</td>\n",
       "      <td>-670.00</td>\n",
       "      <td>-739.000</td>\n",
       "      <td>-639.00</td>\n",
       "      <td>-700.00</td>\n",
       "      <td>-689.00</td>\n",
       "      <td>-718.00</td>\n",
       "      <td>...</td>\n",
       "      <td>34.6</td>\n",
       "      <td>202.00</td>\n",
       "      <td>202.00</td>\n",
       "      <td>261.00</td>\n",
       "      <td>261.00</td>\n",
       "      <td>135.00</td>\n",
       "      <td>135.00</td>\n",
       "      <td>-29.9</td>\n",
       "      <td>-85.2</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>-681.000</td>\n",
       "      <td>-650.000</td>\n",
       "      <td>-681.000</td>\n",
       "      <td>-651.00</td>\n",
       "      <td>-650.00</td>\n",
       "      <td>-599.000</td>\n",
       "      <td>-653.00</td>\n",
       "      <td>-580.00</td>\n",
       "      <td>-578.00</td>\n",
       "      <td>-574.00</td>\n",
       "      <td>...</td>\n",
       "      <td>382.0</td>\n",
       "      <td>312.00</td>\n",
       "      <td>312.00</td>\n",
       "      <td>302.00</td>\n",
       "      <td>302.00</td>\n",
       "      <td>314.00</td>\n",
       "      <td>314.00</td>\n",
       "      <td>-489.0</td>\n",
       "      <td>-387.0</td>\n",
       "      <td>-284.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2130</th>\n",
       "      <td>-924.000</td>\n",
       "      <td>-933.000</td>\n",
       "      <td>-759.000</td>\n",
       "      <td>-924.00</td>\n",
       "      <td>-926.00</td>\n",
       "      <td>-933.000</td>\n",
       "      <td>-754.00</td>\n",
       "      <td>-835.00</td>\n",
       "      <td>-833.00</td>\n",
       "      <td>-813.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-87.5</td>\n",
       "      <td>2.48</td>\n",
       "      <td>2.48</td>\n",
       "      <td>25.60</td>\n",
       "      <td>25.60</td>\n",
       "      <td>-62.10</td>\n",
       "      <td>-62.10</td>\n",
       "      <td>384.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>327.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2131</th>\n",
       "      <td>1.150</td>\n",
       "      <td>7.350</td>\n",
       "      <td>1.150</td>\n",
       "      <td>8.50</td>\n",
       "      <td>8.76</td>\n",
       "      <td>8.970</td>\n",
       "      <td>1.27</td>\n",
       "      <td>7.35</td>\n",
       "      <td>6.97</td>\n",
       "      <td>10.70</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>5.49</td>\n",
       "      <td>5.49</td>\n",
       "      <td>-8.15</td>\n",
       "      <td>-8.15</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>156.0</td>\n",
       "      <td>91.8</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2132 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      min_2_a  min_2_b  min_q_7_a  min_q_17_a  min_q_7_b  min_q_17_b  \\\n",
       "0    -727.000 -727.000   -676.000     -727.00    -727.00    -658.000   \n",
       "1      -0.705   12.700     -0.705       12.00      12.70      13.500   \n",
       "2    -844.000 -814.000   -844.000     -822.00    -803.00    -743.000   \n",
       "3    -819.000 -820.000   -813.000     -712.00    -718.00    -552.000   \n",
       "4      -9.640  -36.000     -5.480       -9.64     -12.30       7.600   \n",
       "...       ...      ...        ...         ...        ...         ...   \n",
       "2127    7.940   -0.538      9.940        8.16       8.89      -0.538   \n",
       "2128 -756.000 -739.000   -756.000     -674.00    -670.00    -739.000   \n",
       "2129 -681.000 -650.000   -681.000     -651.00    -650.00    -599.000   \n",
       "2130 -924.000 -933.000   -759.000     -924.00    -926.00    -933.000   \n",
       "2131    1.150    7.350      1.150        8.50       8.76       8.970   \n",
       "\n",
       "      min_q_2_a  min_q_2_b  min_q_12_a  min_q_12_b  ...  fft_409_a  fft_1_b  \\\n",
       "0       -677.00    -702.00     -698.00     -700.00  ...      445.0   438.00   \n",
       "1         11.10      16.00       16.30       17.70  ...       -4.2    -4.00   \n",
       "2       -707.00    -814.00     -826.00     -610.00  ...      282.0   361.00   \n",
       "3       -671.00    -820.00     -819.00     -584.00  ...      -28.4     3.89   \n",
       "4         12.20      19.40       20.10      -36.00  ...      -55.6   -27.40   \n",
       "...         ...        ...         ...         ...  ...        ...      ...   \n",
       "2127       7.94      11.60       11.20        5.95  ...       27.3     3.37   \n",
       "2128    -639.00    -700.00     -689.00     -718.00  ...       34.6   202.00   \n",
       "2129    -653.00    -580.00     -578.00     -574.00  ...      382.0   312.00   \n",
       "2130    -754.00    -835.00     -833.00     -813.00  ...      -87.5     2.48   \n",
       "2131       1.27       7.35        6.97       10.70  ...      -11.0     5.49   \n",
       "\n",
       "      fft_4_b  fft_4_a  fft_1_a  fft_409_b  fft_406_b  fft_360_a  fft_315_a  \\\n",
       "0      438.00   398.00   398.00     508.00     508.00     -508.0     -359.0   \n",
       "1       -4.00    -4.30    -4.30      -5.64      -5.64      191.0      140.0   \n",
       "2      361.00   204.00   204.00     131.00     131.00      -57.2     -582.0   \n",
       "3        3.89     6.25     6.25       3.32       3.32      217.0      188.0   \n",
       "4      -27.40    -2.37    -2.37      56.40      56.40      350.0      203.0   \n",
       "...       ...      ...      ...        ...        ...        ...        ...   \n",
       "2127     3.37    12.00    12.00      -4.19      -4.19      142.0      112.0   \n",
       "2128   202.00   261.00   261.00     135.00     135.00      -29.9      -85.2   \n",
       "2129   312.00   302.00   302.00     314.00     314.00     -489.0     -387.0   \n",
       "2130     2.48    25.60    25.60     -62.10     -62.10      384.0      119.0   \n",
       "2131     5.49    -8.15    -8.15       3.00       3.00      156.0       91.8   \n",
       "\n",
       "      fft_690_a  \n",
       "0        -462.0  \n",
       "1          76.0  \n",
       "2        -125.0  \n",
       "3          10.0  \n",
       "4         106.0  \n",
       "...         ...  \n",
       "2127      187.0  \n",
       "2128      101.0  \n",
       "2129     -284.0  \n",
       "2130      327.0  \n",
       "2131      159.0  \n",
       "\n",
       "[2132 rows x 75 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded[top_75_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Praveen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from tensorflow.keras.layers import LSTM\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Praveen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9141 - loss: 0.2325 \n",
      "Accuracy : 0.9156908392906189\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Confusion Matrix Accuracy: \n",
      " [[129  19   0]\n",
      " [  1 125  10]\n",
      " [  0   6 137]]\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=input_shape),\n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(y_train.shape[1], activation='softmax') \n",
    "    ])\n",
    "    return model\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = get_cnn_model(input_shape)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), verbose=0)\n",
    "model.save(\"../../Models/EEG/cnnmodel.h5\")\n",
    "score, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Accuracy : {acc}\")\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "predict_classes = np.argmax(pred, axis=1)\n",
    "expected_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "conf_matrix = confusion_matrix(expected_classes, predict_classes)\n",
    "print(\"Confusion Matrix Accuracy: \\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Random Record Index: 216\n",
      "Predicted Class: 0\n",
      "Original Class: 0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random_index = random.randint(0, X_test.shape[0] - 1)\n",
    "random_sample = X_test[random_index].reshape(1, 1, X_test.shape[2])\n",
    "\n",
    "predicted_probabilities = model.predict(random_sample)\n",
    "predicted_class = np.argmax(predicted_probabilities, axis=1)[0]  # Convert prediction to class label\n",
    "\n",
    "# Get the original class for the selected record\n",
    "original_class = np.argmax(y_test[random_index])\n",
    "\n",
    "# Print predicted and original results\n",
    "print(f\"Random Record Index: {random_index}\")\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "print(f\"Original Class: {original_class}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Praveen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9130 - loss: 0.2237 \n",
      "Accuracy after Fine tuning Model 0.908665120601654\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "Confusion Matrix Accuracy: \n",
      " [[127  21   0]\n",
      " [  0 122  14]\n",
      " [  0   4 139]]\n"
     ]
    }
   ],
   "source": [
    "def get_LSTM_model():\n",
    "    model = Sequential([\n",
    "        LSTM(64, input_shape=(1,75),activation=\"relu\",return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32,activation=\"relu\",return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(16,activation=\"relu\"),\n",
    "        Dropout(0.2),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = get_LSTM_model()\n",
    "\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs = 20, validation_data= (X_test, y_test), verbose = 0)\n",
    "model.save(\"../../Models/EEG/lstmmodel.h5\")\n",
    "score, acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Accuracy after Fine tuning Model {acc}\")\n",
    "\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "predict_classes = np.argmax(pred,axis=1)\n",
    "expected_classes = np.argmax(y_test,axis=1)\n",
    "\n",
    "conf_matrix = confusion_matrix(expected_classes,predict_classes)\n",
    "print(\"Confusion Matrix Accuracy: \\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM: 0.9086651053864169\n",
      "Confusion Matrix of SVM:\n",
      " [[133  15   0]\n",
      " [  5 118  13]\n",
      " [  1   5 137]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=1)\n",
    "svm_model.fit(X_train.reshape(X_train.shape[0], -1), np.argmax(y_train, axis=1))\n",
    "joblib.dump(svm_model,\"../../Models/EEG/svmmodel.joblib\")\n",
    "y_pred_svm = svm_model.predict(X_test.reshape(X_test.shape[0], -1))\n",
    "\n",
    "accuracy_svm = accuracy_score(np.argmax(y_test, axis=1), y_pred_svm)\n",
    "conf_matrix_svm = confusion_matrix(np.argmax(y_test, axis=1), y_pred_svm)\n",
    "\n",
    "print(f\"Accuracy of SVM: {accuracy_svm}\")\n",
    "print(\"Confusion Matrix of SVM:\\n\", conf_matrix_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree: 0.9039812646370023\n",
      "Confusion Matrix of Decision Tree:\n",
      " [[139   9   0]\n",
      " [  6 120  10]\n",
      " [  0  16 127]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "decision_tree_model = DecisionTreeClassifier()\n",
    "decision_tree_model.fit(X_train.reshape(X_train.shape[0], -1), np.argmax(y_train, axis=1))\n",
    "joblib.dump(decision_tree_model,\"../../Models/EEG/dtmodel.joblib\")\n",
    "y_pred_dt = decision_tree_model.predict(X_test.reshape(X_test.shape[0], -1))\n",
    "\n",
    "accuracy_dt = accuracy_score(np.argmax(y_test, axis=1), y_pred_dt)\n",
    "conf_matrix_dt = confusion_matrix(np.argmax(y_test, axis=1), y_pred_dt)\n",
    "\n",
    "print(f\"Accuracy of Decision Tree: {accuracy_dt}\")\n",
    "print(\"Confusion Matrix of Decision Tree:\\n\", conf_matrix_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest: 0.9508196721311475\n",
      "Confusion Matrix of Random Forest:\n",
      " [[143   5   0]\n",
      " [  1 123  12]\n",
      " [  0   3 140]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "random_forest_model.fit(X_train.reshape(X_train.shape[0], -1), np.argmax(y_train, axis=1))\n",
    "\n",
    "y_pred_rf = random_forest_model.predict(X_test.reshape(X_test.shape[0], -1))\n",
    "\n",
    "accuracy_rf = accuracy_score(np.argmax(y_test, axis=1), y_pred_rf)\n",
    "conf_matrix_rf = confusion_matrix(np.argmax(y_test, axis=1), y_pred_rf)\n",
    "\n",
    "print(f\"Accuracy of Random Forest: {accuracy_rf}\")\n",
    "print(\"Confusion Matrix of Random Forest:\\n\", conf_matrix_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 1.1107 - val_loss: 1.1333\n",
      "Epoch 2/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.7836 - val_loss: 1.0048\n",
      "Epoch 3/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.7292 - val_loss: 0.9064\n",
      "Epoch 4/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7320 - val_loss: 0.8483\n",
      "Epoch 5/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.7305 - val_loss: 0.8090\n",
      "Epoch 6/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6986 - val_loss: 0.7859\n",
      "Epoch 7/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6458 - val_loss: 0.7644\n",
      "Epoch 8/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6768 - val_loss: 0.7488\n",
      "Epoch 9/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6750 - val_loss: 0.7361\n",
      "Epoch 10/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6676 - val_loss: 0.7265\n",
      "Epoch 11/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6985 - val_loss: 0.7213\n",
      "Epoch 12/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6873 - val_loss: 0.7144\n",
      "Epoch 13/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6594 - val_loss: 0.7022\n",
      "Epoch 14/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6545 - val_loss: 0.7002\n",
      "Epoch 15/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6829 - val_loss: 0.7022\n",
      "Epoch 16/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6893 - val_loss: 0.6964\n",
      "Epoch 17/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6591 - val_loss: 0.6945\n",
      "Epoch 18/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6861 - val_loss: 0.6899\n",
      "Epoch 19/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6976 - val_loss: 0.6882\n",
      "Epoch 20/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6609 - val_loss: 0.6881\n",
      "Epoch 21/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6840 - val_loss: 0.6861\n",
      "Epoch 22/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6335 - val_loss: 0.6864\n",
      "Epoch 23/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6540 - val_loss: 0.6856\n",
      "Epoch 24/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6623 - val_loss: 0.6840\n",
      "Epoch 25/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6560 - val_loss: 0.6876\n",
      "Epoch 26/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6713 - val_loss: 0.6836\n",
      "Epoch 27/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6537 - val_loss: 0.6828\n",
      "Epoch 28/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6370 - val_loss: 0.6833\n",
      "Epoch 29/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6250 - val_loss: 0.6833\n",
      "Epoch 30/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6276 - val_loss: 0.6799\n",
      "Epoch 31/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6788 - val_loss: 0.6786\n",
      "Epoch 32/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6556 - val_loss: 0.6775\n",
      "Epoch 33/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6274 - val_loss: 0.6776\n",
      "Epoch 34/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6115 - val_loss: 0.6792\n",
      "Epoch 35/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6424 - val_loss: 0.6823\n",
      "Epoch 36/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6378 - val_loss: 0.6747\n",
      "Epoch 37/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6577 - val_loss: 0.6744\n",
      "Epoch 38/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6369 - val_loss: 0.6751\n",
      "Epoch 39/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6552 - val_loss: 0.6740\n",
      "Epoch 40/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6254 - val_loss: 0.6751\n",
      "Epoch 41/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6159 - val_loss: 0.6739\n",
      "Epoch 42/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6601 - val_loss: 0.6725\n",
      "Epoch 43/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6342 - val_loss: 0.6729\n",
      "Epoch 44/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6207 - val_loss: 0.6725\n",
      "Epoch 45/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6637 - val_loss: 0.6728\n",
      "Epoch 46/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6277 - val_loss: 0.6721\n",
      "Epoch 47/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6363 - val_loss: 0.6718\n",
      "Epoch 48/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6415 - val_loss: 0.6722\n",
      "Epoch 49/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6306 - val_loss: 0.6730\n",
      "Epoch 50/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6301 - val_loss: 0.6719\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Praveen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.4283 - loss: 1.6197 - val_accuracy: 0.7494 - val_loss: 0.6836\n",
      "Epoch 2/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6114 - loss: 0.9946 - val_accuracy: 0.7963 - val_loss: 0.5403\n",
      "Epoch 3/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7109 - loss: 0.7761 - val_accuracy: 0.8337 - val_loss: 0.4618\n",
      "Epoch 4/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7654 - loss: 0.6284 - val_accuracy: 0.8407 - val_loss: 0.4225\n",
      "Epoch 5/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7691 - loss: 0.5796 - val_accuracy: 0.8501 - val_loss: 0.3724\n",
      "Epoch 6/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7841 - loss: 0.5847 - val_accuracy: 0.8478 - val_loss: 0.3530\n",
      "Epoch 7/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8024 - loss: 0.5174 - val_accuracy: 0.8478 - val_loss: 0.3356\n",
      "Epoch 8/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8313 - loss: 0.4561 - val_accuracy: 0.8501 - val_loss: 0.3291\n",
      "Epoch 9/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8347 - loss: 0.4565 - val_accuracy: 0.8548 - val_loss: 0.3156\n",
      "Epoch 10/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8392 - loss: 0.4415 - val_accuracy: 0.8735 - val_loss: 0.3006\n",
      "Epoch 11/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8485 - loss: 0.3871 - val_accuracy: 0.8829 - val_loss: 0.2884\n",
      "Epoch 12/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8313 - loss: 0.4566 - val_accuracy: 0.8782 - val_loss: 0.2834\n",
      "Epoch 13/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8585 - loss: 0.3822 - val_accuracy: 0.8782 - val_loss: 0.2788\n",
      "Epoch 14/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8452 - loss: 0.3875 - val_accuracy: 0.8852 - val_loss: 0.2710\n",
      "Epoch 15/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8602 - loss: 0.3675 - val_accuracy: 0.8759 - val_loss: 0.2752\n",
      "Epoch 16/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8548 - loss: 0.3544 - val_accuracy: 0.8852 - val_loss: 0.2662\n",
      "Epoch 17/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8810 - loss: 0.3119 - val_accuracy: 0.8806 - val_loss: 0.2654\n",
      "Epoch 18/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8750 - loss: 0.3234 - val_accuracy: 0.8829 - val_loss: 0.2550\n",
      "Epoch 19/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8681 - loss: 0.3356 - val_accuracy: 0.8806 - val_loss: 0.2562\n",
      "Epoch 20/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8918 - loss: 0.3092 - val_accuracy: 0.8782 - val_loss: 0.2589\n",
      "Epoch 21/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8959 - loss: 0.2894 - val_accuracy: 0.8759 - val_loss: 0.2668\n",
      "Epoch 22/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8735 - loss: 0.3078 - val_accuracy: 0.8759 - val_loss: 0.2599\n",
      "Epoch 23/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8777 - loss: 0.3059 - val_accuracy: 0.8829 - val_loss: 0.2595\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8995 - loss: 0.2399 \n",
      "Classifier Accuracy: 0.8829039931297302\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Confusion Matrix:\n",
      " [[130  18   0]\n",
      " [  6 107  23]\n",
      " [  0   3 140]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, MaxPooling1D, UpSampling1D, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def add_noise(data, noise_factor=0.2):  \n",
    "    noisy_data = data + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=data.shape)\n",
    "    return np.clip(noisy_data, 0., 1.)\n",
    "\n",
    "def get_denoising_autoencoder(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    \n",
    "    x = Conv1D(64, 3, activation='relu', padding='same')(input_layer)\n",
    "    x = BatchNormalization()(x)  \n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Conv1D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    encoded = MaxPooling1D(2, padding='same')(x)\n",
    "\n",
    "    x = Conv1D(128, 3, activation='relu', padding='same')(encoded)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = UpSampling1D(2)(x)\n",
    "    x = Conv1D(64, 3, activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = UpSampling1D(2)(x)\n",
    "    decoded = Conv1D(input_shape[-1], 3, activation='sigmoid', padding='same')(x)\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    return autoencoder\n",
    "\n",
    "def get_classifier_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=input_shape),\n",
    "        Dense(128, activation='relu'),  \n",
    "        BatchNormalization(),  \n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),  \n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(3, activation='softmax')  \n",
    "    ])\n",
    "    return model\n",
    "\n",
    "X_train_noisy = add_noise(X_train)\n",
    "X_test_noisy = add_noise(X_test)\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "autoencoder = get_denoising_autoencoder(input_shape)\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')  \n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "autoencoder.fit(X_train_noisy, X_train, epochs=50, batch_size=64, validation_data=(X_test_noisy, X_test), \n",
    "                callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "encoder = Model(inputs=autoencoder.input, outputs=autoencoder.layers[4].output)  \n",
    "\n",
    "X_train_encoded = encoder.predict(X_train)\n",
    "X_test_encoded = encoder.predict(X_test)\n",
    "\n",
    "classifier = get_classifier_model(X_train_encoded.shape[1:])\n",
    "classifier.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "classifier.fit(X_train_encoded, y_train, epochs=50, batch_size=64, validation_data=(X_test_encoded, y_test), \n",
    "               callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "score, acc = classifier.evaluate(X_test_encoded, y_test)\n",
    "print(f\"Classifier Accuracy: {acc}\")\n",
    "\n",
    "y_pred = classifier.predict(X_test_encoded)\n",
    "predict_classes = np.argmax(y_pred, axis=1)\n",
    "expected_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "conf_matrix = confusion_matrix(expected_classes, predict_classes)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Praveen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5033 - loss: 1.8109 - val_accuracy: 0.8431 - val_loss: 1.2237\n",
      "Epoch 2/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7685 - loss: 1.1819 - val_accuracy: 0.8478 - val_loss: 1.0163\n",
      "Epoch 3/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7974 - loss: 1.1338 - val_accuracy: 0.8548 - val_loss: 0.9411\n",
      "Epoch 4/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8211 - loss: 1.0390 - val_accuracy: 0.8665 - val_loss: 0.8895\n",
      "Epoch 5/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8422 - loss: 0.9568 - val_accuracy: 0.8642 - val_loss: 0.8463\n",
      "Epoch 6/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8465 - loss: 0.9966 - val_accuracy: 0.8782 - val_loss: 0.8350\n",
      "Epoch 7/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8476 - loss: 0.9250 - val_accuracy: 0.8806 - val_loss: 0.8118\n",
      "Epoch 8/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8500 - loss: 0.9227 - val_accuracy: 0.8923 - val_loss: 0.7954\n",
      "Epoch 9/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8406 - loss: 0.9053 - val_accuracy: 0.8923 - val_loss: 0.7929\n",
      "Epoch 10/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8717 - loss: 0.8413 - val_accuracy: 0.8899 - val_loss: 0.7693\n",
      "Epoch 11/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8671 - loss: 0.8470 - val_accuracy: 0.8946 - val_loss: 0.7671\n",
      "Epoch 12/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8690 - loss: 0.8446 - val_accuracy: 0.8970 - val_loss: 0.7494\n",
      "Epoch 13/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8744 - loss: 0.8322 - val_accuracy: 0.8899 - val_loss: 0.7612\n",
      "Epoch 14/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8685 - loss: 0.8116 - val_accuracy: 0.9016 - val_loss: 0.7339\n",
      "Epoch 15/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8757 - loss: 0.7770 - val_accuracy: 0.8899 - val_loss: 0.7307\n",
      "Epoch 16/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8756 - loss: 0.7809 - val_accuracy: 0.8993 - val_loss: 0.7166\n",
      "Epoch 17/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8902 - loss: 0.7408 - val_accuracy: 0.9040 - val_loss: 0.7004\n",
      "Epoch 18/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9071 - loss: 0.7172 - val_accuracy: 0.8829 - val_loss: 0.7270\n",
      "Epoch 19/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9122 - loss: 0.7071 - val_accuracy: 0.8946 - val_loss: 0.7204\n",
      "Epoch 20/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8986 - loss: 0.6969 - val_accuracy: 0.9087 - val_loss: 0.6852\n",
      "Epoch 21/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9006 - loss: 0.7027 - val_accuracy: 0.8876 - val_loss: 0.6928\n",
      "Epoch 22/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9147 - loss: 0.6708 - val_accuracy: 0.9016 - val_loss: 0.6635\n",
      "Epoch 23/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9040 - loss: 0.6724 - val_accuracy: 0.8923 - val_loss: 0.6626\n",
      "Epoch 24/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9042 - loss: 0.6649 - val_accuracy: 0.8876 - val_loss: 0.6791\n",
      "Epoch 25/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9046 - loss: 0.6729 - val_accuracy: 0.8970 - val_loss: 0.6546\n",
      "Epoch 26/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9047 - loss: 0.6543 - val_accuracy: 0.9063 - val_loss: 0.6275\n",
      "Epoch 27/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9181 - loss: 0.6167 - val_accuracy: 0.8970 - val_loss: 0.6289\n",
      "Epoch 28/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9113 - loss: 0.6153 - val_accuracy: 0.8993 - val_loss: 0.6162\n",
      "Epoch 29/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9011 - loss: 0.6264 - val_accuracy: 0.8876 - val_loss: 0.6577\n",
      "Epoch 30/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9136 - loss: 0.6048 - val_accuracy: 0.8993 - val_loss: 0.6112\n",
      "Epoch 31/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9139 - loss: 0.6083 - val_accuracy: 0.9040 - val_loss: 0.5837\n",
      "Epoch 32/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9308 - loss: 0.5550 - val_accuracy: 0.9040 - val_loss: 0.5894\n",
      "Epoch 33/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8999 - loss: 0.6127 - val_accuracy: 0.8993 - val_loss: 0.5737\n",
      "Epoch 34/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9336 - loss: 0.5291 - val_accuracy: 0.9016 - val_loss: 0.5842\n",
      "Epoch 35/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9082 - loss: 0.5826 - val_accuracy: 0.8970 - val_loss: 0.5738\n",
      "Epoch 36/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9153 - loss: 0.5469 - val_accuracy: 0.8970 - val_loss: 0.5555\n",
      "Epoch 37/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9157 - loss: 0.5404 - val_accuracy: 0.9157 - val_loss: 0.5411\n",
      "Epoch 38/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9046 - loss: 0.5725 - val_accuracy: 0.8970 - val_loss: 0.5593\n",
      "Epoch 39/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9234 - loss: 0.5240 - val_accuracy: 0.9063 - val_loss: 0.5243\n",
      "Epoch 40/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9301 - loss: 0.5344 - val_accuracy: 0.9063 - val_loss: 0.5188\n",
      "Epoch 41/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9178 - loss: 0.4943 - val_accuracy: 0.8993 - val_loss: 0.5350\n",
      "Epoch 42/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9384 - loss: 0.4798 - val_accuracy: 0.9063 - val_loss: 0.5186\n",
      "Epoch 43/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9258 - loss: 0.4749 - val_accuracy: 0.9087 - val_loss: 0.5107\n",
      "Epoch 44/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9413 - loss: 0.4620 - val_accuracy: 0.8946 - val_loss: 0.5178\n",
      "Epoch 45/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9185 - loss: 0.4923 - val_accuracy: 0.9180 - val_loss: 0.4884\n",
      "Epoch 46/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9258 - loss: 0.4898 - val_accuracy: 0.9133 - val_loss: 0.4998\n",
      "Epoch 47/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9231 - loss: 0.4835 - val_accuracy: 0.9063 - val_loss: 0.4972\n",
      "Epoch 48/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9133 - loss: 0.4866 - val_accuracy: 0.8946 - val_loss: 0.5177\n",
      "Epoch 49/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9315 - loss: 0.4508 - val_accuracy: 0.8970 - val_loss: 0.4963\n",
      "Epoch 50/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9171 - loss: 0.4441 - val_accuracy: 0.8946 - val_loss: 0.4997\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9271 - loss: 0.4726 \n",
      "Classifier Accuracy: 0.9180327653884888\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Confusion Matrix:\n",
      " [[140   8   0]\n",
      " [  8 112  16]\n",
      " [  0   3 140]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import LeakyReLU, Add, Input, Dense, Dropout, BatchNormalization, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def get_complex_classifier_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Flatten()(inputs)\n",
    "    x = Dense(256, kernel_regularizer=l2(0.001))(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x1 = Dense(128, kernel_regularizer=l2(0.001))(x)\n",
    "    x1 = LeakyReLU(alpha=0.1)(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x2 = Dense(128, kernel_regularizer=l2(0.001))(x1)\n",
    "    x2 = LeakyReLU(alpha=0.1)(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "\n",
    "    x = Add()([x1, x2])\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Dense(64, kernel_regularizer=l2(0.001))(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    outputs = Dense(3, activation='softmax')(x)  \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "complex_classifier = get_complex_classifier_model(X_train_encoded.shape[1:])\n",
    "complex_classifier.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "complex_classifier.fit(X_train_encoded, y_train, epochs=50, batch_size=64, validation_data=(X_test_encoded, y_test), \n",
    "                       callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "score, acc = complex_classifier.evaluate(X_test_encoded, y_test)\n",
    "print(f\"Classifier Accuracy: {acc}\")\n",
    "\n",
    "y_pred = complex_classifier.predict(X_test_encoded)\n",
    "predict_classes = np.argmax(y_pred, axis=1)\n",
    "expected_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "conf_matrix = confusion_matrix(expected_classes, predict_classes)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
